{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinSeokH/2024_Sejong_AI/blob/main/AI%EA%B3%B5%EB%AA%A8%EC%A0%84(%EC%84%B8%EC%A2%85)_short.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/AI경진대회/segmentation_new\")\n",
        "from unet  import *\n",
        "from dataprocessing  import *"
      ],
      "metadata": {
        "id": "lxwyt7H_nrqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2adc00b7-0109-436d-931f-c33b81cc1675"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 이미지와 레이블맵 파일들의 디렉토리와 파일 확장자 설정\n",
        "dir_data = '/content/drive/MyDrive/AI경진대회/segmentation_new'\n",
        "img_dir = '/content/drive/MyDrive/AI경진대회/segmentation_new/images'\n",
        "labelmap_dir = '/content/drive/MyDrive/AI경진대회/segmentation_new/labels'\n",
        "\n",
        "## train/test/val 폴더 내 파일 개수\n",
        "nframe_train = 24\n",
        "nframe_val = 3\n",
        "nframe_test = 3\n",
        "\n",
        "dir_save_train = os.path.join(dir_data, 'train')\n",
        "dir_save_val = os.path.join(dir_data, 'val')\n",
        "dir_save_test = os.path.join(dir_data, 'test')"
      ],
      "metadata": {
        "id": "uqrK2G_BIJp_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 파라미터 설정하기\n",
        "lr = 1e-3\n",
        "batch_size = 5\n",
        "num_epoch = 2\n",
        "base_dir = '/content/drive/MyDrive/DACrew/unet'\n",
        "data_dir = dir_data\n",
        "ckpt_dir = os.path.join(base_dir, \"checkpoint\")\n",
        "log_dir = os.path.join(base_dir, \"log\")\n",
        "\n",
        "# 훈련을 위한 Transform과 DataLoader\n",
        "transform = transforms.Compose([Normalization(mean=0.5, std=0.5), RandomFlip(), ToTensor()])\n",
        "\n",
        "dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'), transform=transform)\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "dataset_val = Dataset(data_dir=os.path.join(data_dir, 'val'), transform=transform)\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "# 네트워크 생성하기\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "net = UNet().to(device)\n",
        "\n",
        "# 손실함수 정의하기\n",
        "fn_loss = torch.nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# Optimizer 설정하기\n",
        "optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_train = len(dataset_train)\n",
        "num_data_val = len(dataset_val)\n",
        "\n",
        "num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "num_batch_val = np.ceil(num_data_val / batch_size)\n",
        "\n",
        "# 그 밖에 부수적인 functions 설정하기\n",
        "fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0,2,3,1)\n",
        "\n",
        "fn_tonumpy_output = lambda x: x.detach().to('cpu').numpy().transpose(0,2,3,1)\n",
        "\n",
        "fn_denorm = lambda x, mean, std: (x * std) + mean\n",
        "fn_class = lambda x: 1.0 * (x > 0.5)\n",
        "\n",
        "# Tensorboard 를 사용하기 위한 SummaryWriter 설정\n",
        "writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "writer_val = SummaryWriter(log_dir=os.path.join(log_dir, 'val'))\n",
        "\n",
        "# 네트워크 학습시키기\n",
        "st_epoch = 0\n",
        "# 학습한 모델이 있을 경우 모델 로드하기\n",
        "import re\n",
        "\n",
        "def extract_number_from_filename(filename):\n",
        "    numbers = re.findall(r'\\d+', filename)\n",
        "    return int(numbers[0]) if numbers else 0\n",
        "\n",
        "# ckpt_lst.sort(key=lambda f: extract_number_from_filename(f))\n",
        "# net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n"
      ],
      "metadata": {
        "id": "Btf9g_sdB8TV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(st_epoch + 1, num_epoch + 1):\n",
        "        net.train()\n",
        "        loss_arr = []\n",
        "\n",
        "        for batch, data in enumerate(loader_train, 1):\n",
        "            # forward pass\n",
        "            label = data['label'].to(device)\n",
        "            input = data['input'].to(device)\n",
        "            output = net(input)\n",
        "\n",
        "            # backward pass\n",
        "            optim.zero_grad()\n",
        "            loss = fn_loss(output, label.argmax(dim=1))\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "            _, output = torch.max(output, 1)\n",
        "\n",
        "            # 손실함수 계산\n",
        "            loss_arr += [loss.item()]\n",
        "            print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n",
        "                  (epoch, num_epoch, batch, num_batch_train, np.mean(loss_arr)))\n",
        "\n",
        "            # Tensorboard 저장하기\n",
        "            label = fn_tonumpy(label)\n",
        "            label = np.argmax(label, axis=-1)\n",
        "            label = np.expand_dims(label, axis=-1)\n",
        "\n",
        "            input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "            output = torch.unsqueeze(output, axis = -3 )\n",
        "            output = fn_tonumpy_output(output) # fn_class 없애줌\n",
        "\n",
        "            writer_train.add_image('label', label, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_train.add_image('input', input, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "            writer_train.add_image('output', output, num_batch_train * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        writer_train.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            net.eval()\n",
        "            loss_arr = []\n",
        "\n",
        "            for batch, data in enumerate(loader_val, 1):\n",
        "                # forward pass\n",
        "                label = data['label'].to(device)\n",
        "                input = data['input'].to(device)\n",
        "\n",
        "                output = net(input)\n",
        "\n",
        "                # 손실함수 계산하기\n",
        "                loss = fn_loss(output, label.argmax(dim=1))\n",
        "                _, output = torch.max(output, 1)\n",
        "                loss_arr += [loss.item()]\n",
        "                print(\"VALID: EPOCH %04d / %04d | BATCH %04d / %04d | LOSS %.4f\" %\n",
        "                      (epoch, num_epoch, batch, num_batch_val, np.mean(loss_arr)))\n",
        "\n",
        "                # Tensorboard 저장하기\n",
        "\n",
        "                label = fn_tonumpy(label)\n",
        "                label = np.argmax(label, axis=-1)\n",
        "                label = np.expand_dims(label, axis=-1)\n",
        "\n",
        "                input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "                output = torch.unsqueeze(output, axis = -3 )\n",
        "                output = fn_tonumpy_output(output)\n",
        "\n",
        "                writer_val.add_image('label', label, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n",
        "                writer_val.add_image('input', input, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n",
        "                writer_val.add_image('output', output, num_batch_val * (epoch - 1) + batch, dataformats='NHWC')\n",
        "\n",
        "        writer_val.add_scalar('loss', np.mean(loss_arr), epoch)\n",
        "\n",
        "        # epoch 50마다 모델 저장하기\n",
        "        if epoch % 71 == 0:\n",
        "            save(ckpt_dir=ckpt_dir, net=net, optim=optim, epoch=epoch)\n",
        "\n",
        "        writer_train.close()\n",
        "        writer_val.close()"
      ],
      "metadata": {
        "id": "3c40sP4B2ky0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6377ea1e-9629-48b6-f4ed-a2399e4f5804"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN: EPOCH 0001 / 0002 | BATCH 0001 / 0005 | LOSS 1.3280\n",
            "TRAIN: EPOCH 0001 / 0002 | BATCH 0002 / 0005 | LOSS 1.3050\n",
            "TRAIN: EPOCH 0001 / 0002 | BATCH 0003 / 0005 | LOSS 1.2420\n",
            "TRAIN: EPOCH 0001 / 0002 | BATCH 0004 / 0005 | LOSS 1.1986\n",
            "TRAIN: EPOCH 0001 / 0002 | BATCH 0005 / 0005 | LOSS 1.1458\n",
            "VALID: EPOCH 0001 / 0002 | BATCH 0001 / 0001 | LOSS 1.2728\n",
            "TRAIN: EPOCH 0002 / 0002 | BATCH 0001 / 0005 | LOSS 0.8348\n",
            "TRAIN: EPOCH 0002 / 0002 | BATCH 0002 / 0005 | LOSS 0.8021\n",
            "TRAIN: EPOCH 0002 / 0002 | BATCH 0003 / 0005 | LOSS 0.7772\n",
            "TRAIN: EPOCH 0002 / 0002 | BATCH 0004 / 0005 | LOSS 0.7567\n",
            "TRAIN: EPOCH 0002 / 0002 | BATCH 0005 / 0005 | LOSS 0.7340\n",
            "VALID: EPOCH 0002 / 0002 | BATCH 0001 / 0001 | LOSS 1.2287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([Normalization(mean=0.5, std=0.5), ToTensor()])\n",
        "\n",
        "dataset_test = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform)\n",
        "loader_test = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "# 그밖에 부수적인 variables 설정하기\n",
        "num_data_test = len(dataset_test)\n",
        "num_batch_test = np.ceil(num_data_test / batch_size)\n",
        "\n",
        "# 결과 디렉토리 생성하기\n",
        "result_dir = os.path.join(base_dir, 'result_new_test')\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(os.path.join(result_dir, 'png'))\n",
        "    os.makedirs(os.path.join(result_dir, 'numpy'))\n",
        "\n",
        "\n",
        "# net, optim, st_epoch = load(ckpt_dir=ckpt_dir, net=net, optim=optim)\n",
        "\n",
        "with torch.no_grad():\n",
        "      net.eval()\n",
        "      loss_arr = []\n",
        "\n",
        "      for batch, data in enumerate(loader_test, 1):\n",
        "          # forward pass\n",
        "          label = data['label'].to(device)\n",
        "          input = data['input'].to(device)\n",
        "\n",
        "          output = net(input)\n",
        "\n",
        "          # 손실함수 계산하기\n",
        "          loss = fn_loss(output, label)\n",
        "          _, output = torch.max(output, 1)\n",
        "\n",
        "          loss_arr += [loss.item()]\n",
        "\n",
        "          print(\"TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
        "                (batch, num_batch_test, np.mean(loss_arr)))\n",
        "\n",
        "          # Tensorboard 저장하기\n",
        "\n",
        "          label = fn_tonumpy(label)\n",
        "          label = np.argmax(label, axis=-1)\n",
        "          label = np.expand_dims(label, axis=-1)\n",
        "          input = fn_tonumpy(fn_denorm(input, mean=0.5, std=0.5))\n",
        "          output = torch.unsqueeze(output, axis = -3 )\n",
        "          output = fn_tonumpy_output(output)\n",
        "\n",
        "          # 테스트 결과 저장하기\n",
        "          for j in range(label.shape[0]):\n",
        "              id = num_batch_test * (batch - 1) + j\n",
        "\n",
        "              plt.imsave(os.path.join(result_dir, 'png', 'label_%04d.png' % id), label[j].squeeze(), cmap='gray')\n",
        "              plt.imsave(os.path.join(result_dir, 'png', 'input_%04d.png' % id), input[j].squeeze(), cmap='gray')\n",
        "              plt.imsave(os.path.join(result_dir, 'png', 'output_%04d.png' % id), output[j].squeeze(), cmap='gray')\n",
        "\n",
        "              np.save(os.path.join(result_dir, 'numpy', 'label_%04d.npy' % id), label[j].squeeze())\n",
        "              np.save(os.path.join(result_dir, 'numpy', 'input_%04d.npy' % id), input[j].squeeze())\n",
        "              np.save(os.path.join(result_dir, 'numpy', 'output_%04d.npy' % id), output[j].squeeze())\n",
        "\n",
        "print(\"AVERAGE TEST: BATCH %04d / %04d | LOSS %.4f\" %\n",
        "        (batch, num_batch_test, np.mean(loss_arr)))"
      ],
      "metadata": {
        "id": "yI7OvSGG2nE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "lst_data = os.listdir(os.path.join(result_dir, 'numpy'))\n",
        "\n",
        "lst_label = [f for f in lst_data if f.startswith('label')]\n",
        "lst_input = [f for f in lst_data if f.startswith('input')]\n",
        "lst_output = [f for f in lst_data if f.startswith('output')]\n",
        "\n",
        "lst_label.sort()\n",
        "lst_input.sort()\n",
        "lst_output.sort()\n",
        "\n",
        "##\n",
        "id = 2\n",
        "\n",
        "label = np.load(os.path.join(result_dir,\"numpy\", lst_label[id]))\n",
        "input = np.load(os.path.join(result_dir,\"numpy\", lst_input[id]))\n",
        "output = np.load(os.path.join(result_dir,\"numpy\", lst_output[id]))\n",
        "\n",
        "## 플롯 그리기\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.subplot(131)\n",
        "plt.imshow(input)\n",
        "plt.title('input')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(label)\n",
        "plt.title('label')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(output)\n",
        "print(output.min())\n",
        "plt.title('output')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xCrahUAl2ojL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "bsAnyS7MFv6Q"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}